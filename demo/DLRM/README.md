Important note: This branch contains some modifications within graph_executor.cc which will be removed in future.

# DEMO setup

The folder DLRM contains a set of scripts which are used for DLRM model inference performance analysis.
Environment setup script:
```
python prepare.py
```

This script does following things:
* Uploads DLRM repository and MLCommons Inference repository.
* Uploads 100 GB DLRM model. The model will be stored in tvm/demo/DLRM/model folder.
* Performs weights extraction for further inference. The converted weights are stored in tvm/demo/DLRM/converted folder

# DRM model tuning

```
python tune_dlrm.py --output-log OUTPUT_LOG --output-folder OUTPUT_FOLDER [--onnx-model ONNX_MODEL]
```

Where:
OUTPUT_LOG - is a name of log file with tuning information.
OUTPUT_FOLDER - folder where generated library and json file will be stored  after the script finished.
ONNX_MODEL - optional parameter which should be used if the model is not located in tvm/demo/DLRM/model folder.

After script execution the OUTPUT_FOLDER will contain following files:
* saved_model_XXX.tar
* saved_model_XXX.tar.so
* model_serialized_tuned_XXX.json
Where XXX is a number of iterations for tuning. The default value for this parameter is 3000 but it can be reconfigured in params_demo.py file.

# Native Inference pipeline

Compile rules:

```
mkdir build
cd ./build
cmake ..
make
```
Command line:
'''
dlrm_infer <reference to model so> <reference to model json> <reference to weights folder> <reference to test data folder>
'''
Where:
<reference to model so> is a result  of DLRM model tuning (see #DLRM model tuning). By default it is OUTPUT_FOLDER/saved_model_XXX.tar.so
<reference to model json> is a result  of DLRM model tuning (see #DLRM model tuning). By default it is OUTPUT_FOLDER/model_serialized_tuned_XXX.json
<reference to weights folder> Model weights folder. By default it should be: tvm/demo/DLRM/converted
<reference to test data folder> Folder with testing data. By default it should be: tvm/demo/DLRM/test_data

# Performance results
AMD HW configuration (from Asure):
  Standard HB120rs v3
  vCPUs 120
  RAM 448 GiB
The preliminary performance results can be found here: https://docs.google.com/spreadsheets/d/1lYob-9J3-EUqFnaC3551zP56TAiQ7iyhzDwXvLyZh90/edit?usp=sharing

# Additional useful scripts
The script which allows to create library and model json files without model tuning:
'''
run_dlrm.py --onnx-file ONNX_FILE --log-file LOG_FILE [--test-data TEST_DATA] [--output-folder OUTPUT_FOLDER]
'''
Where: 
ONNX_FILE - onnx DLRM model
LOG_FILE - generated by tune_dlrm.py script
TEST_DATA - optional, folder with test data. If this variable is not set the reference data is taken from tvm/demo/DLRM/test_data folder.
OUTPUT_FOLDER - optional, folder where generated library and json file will be stored  after the script finished. 

It should be noted that if DNNL backend is enabled some problems can occur with library and json generation 
so the library compilarion is disabled in this script.
