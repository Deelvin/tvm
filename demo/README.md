Important note: This branch contains some modifications within graph_executor.cc which will be removed in future.

# DEMO setup

The folder DLRM contains a set of scripts which are used for DLRM model inference performance analysis.
Environment setup script:
```
python prepare.py
```

This script does following things:
* Uploads MLCommons Inference repository
* Uploads data  for BERT model cnd creates docer container for the BERT model inference.
* Uploads DLRM repository.
* Uploads 100 GB DLRM model. The model will be stored in tvm/demo/DLRM/model folder.
* Performs weights extraction for further inference. The converted weights are stored in tvm/demo/DLRM/converted folder.
* Updates MLCommons Inference repository to run DLRM and BERT models with tvm inference.
* Generates fake dataset for inference.

Note: it is not required to setup envirenment variables according to /inference/recommendation/dlrm/pytorch/README.md because
these variables are set at the beginning of run_local.sh script.

# DLRM model tuning

```
python tune_dlrm.py --output-log OUTPUT_LOG --output-folder OUTPUT_FOLDER [--onnx-model ONNX_MODEL]  [--batch-size BATCH_SIZE]
```

Where:
OUTPUT_LOG - is a name of log file with tuning information.
OUTPUT_FOLDER - folder where generated library and json file will be stored  after the script finished.
ONNX_MODEL - optional parameter which should be used if the model is not located in tvm/demo/DLRM/model folder.
BATCH_SIZE - optional parameter which resets DLRM model batch size for the inference.

After script execution the OUTPUT_FOLDER will contain following files:
* saved_model_XXX.tar
* saved_model_XXX.tar.so
* model_serialized_tuned_XXX.json
Where XXX is a number of iterations for tuning. The default value for this parameter is 3000 but it can be reconfigured in params_demo.py file.

# Native Inference pipeline

Compile rules:

```
mkdir build
cd ./build
cmake ..
make
```
Command line:
```
dlrm_infer <reference to model so> <reference to model json> <reference to weights folder> <reference to test data folder>
```
Where:
<reference to model so> is a result  of DLRM model tuning (see #DLRM model tuning). By default it is OUTPUT_FOLDER/saved_model_XXX.tar.so file.
<reference to model json> is a result  of DLRM model tuning (see #DLRM model tuning). By default it is OUTPUT_FOLDER/model_serialized_tuned_XXX.json file.
<reference to weights folder> Model weights folder. By default it should be: tvm/demo/DLRM/converted folder.
<reference to test data folder> Folder with testing data. By default it should be: tvm/demo/DLRM/test_data  folder.

# Performance results
AMD HW configuration (from Asure):
  Standard HB120rs v3
  vCPUs 120
  RAM 448 GiB
The preliminary performance results can be found here: https://docs.google.com/spreadsheets/d/1lYob-9J3-EUqFnaC3551zP56TAiQ7iyhzDwXvLyZh90/edit?usp=sharing

# Additional useful scripts

## Library creation without tuning
The script which allows to create library and model json files without model tuning:
```
run_dlrm.py --onnx-file ONNX_FILE --log-file LOG_FILE [--test-data TEST_DATA] [--output-folder OUTPUT_FOLDER] [--batch-size BATCH_SIZE]
```
Where: 
ONNX_FILE - onnx DLRM model.
LOG_FILE - generated by tune_dlrm.py script.
TEST_DATA - optional, folder with test data. If this variable is not set the reference data is taken from tvm/demo/DLRM/test_data folder.
OUTPUT_FOLDER - optional, folder where generated library and json file will be stored  after the script finished. 
BATCH_SIZE - optional parameter which resets DLRM model batch size for the inference, the batch size should ve aligned with LOG_FILE internals.

It should be noted that if DNNL backend is enabled some problems can occur with library and json files generation, 
so this generation is disabled within this script.

## Performance test to check influence of OMP_NUM_THREADS value to DLRM model perfromance
The script which automates timings collection over OMP_NUM_THREADS set.
```
timings.sh --onnx-file ONNX_FILE --log-file LOG_FILE
```
Where:
ONNX_FILE - onnx DLRM model.
LOG_FILE - generated by tune_dlrm.py script.

## Tuning test to check influence of OMP_NUM_THREADS value to DLRM model perfromance

```
tuning.sh --output-folder=OUTPUT_FOLDER
```
Where:
OUTPUT_FOLDER - path to the output library and json files.

## Timings data conversion script
```
python parse_timings_log.py --input INPUT --output OUTPUT
```
Where:
INPUT - streamed run_dlrm.py output.
OUTPUT - desired csv file location and name.
